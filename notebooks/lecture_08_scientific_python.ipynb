{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40ef5a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CYPLAN255\n",
    "### Urban Informatics and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceea7a8",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 07 -- Scientific Computing in Python\n",
    "*******\n",
    "February 12, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f134ab8",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://i.redd.it/nmx3oz11h8191.jpg\" title=\"Children in a traditional minobashi raincoat going to a new year's event, Niigata prefecture, Japan 1956 [700 x 1041]\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7d376",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "1. Announcements\n",
    "2. Scientific Computing in Python\n",
    "3. Intro to Data Analysis\n",
    "4. For next time\n",
    "5. Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c0cef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Announcements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d336f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Assignment 2 released tonight\n",
    "2. Github Pages workshop with Meiqing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b61f31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Scientific Computing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b134d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> _\"Scientific computing is the collection of **tools**, **techniques**, and **theories** required to solve on a computer **mathematical models** of **problems in Science and Engineering**\"_\n",
    "\n",
    "<p style='text-align: right;'>-- Gene H. Golub and James M. Ortega. <br><i>Scientific Computing and Differential Equations â€“ An Introduction to Numerical Methods</i>. Academic Press, 1992.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f046415",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.1 Motivating Example: \"Brute Force\" Data Analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab587ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's use the methods we learned to iterate through the rows of a file in order to process each row, one at a time.  We will load it via the `csv` library and open the file and iterate through the rows to compute the `mean`, `max`, and `min` values of rainfall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1687bf32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1.1 Row-based iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e66932",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('data/rain.csv', 'r') as csvfile:  # this is how you open a file in Python\n",
    "    \n",
    "    # initialize a counter and variables to contain our descriptive stats\n",
    "    count = 0  # at the end, divide cumulative_sum by this to get the mean\n",
    "    cumulative_sum = 0  # our rolling sum\n",
    "    max_value = -1  # pick a really small number that's guaranteed to be less than the max\n",
    "    min_value = 1000\n",
    "    \n",
    "    # open the file and skip the header row\n",
    "    my_csv = csv.reader(csvfile)\n",
    "    next(my_csv)\n",
    "    \n",
    "    # loop through each data row\n",
    "    for row in my_csv:\n",
    "        \n",
    "        # rainfall amount is in column 1, only process this row's value if not an empty string\n",
    "        if not row[1] == '':\n",
    "            \n",
    "            # increment the counter and extract this row's rainfall as a float\n",
    "            count = count + 1\n",
    "            rainfall = float(row[1])\n",
    "            \n",
    "            # add this row's rainfall to the cumulative sum\n",
    "            cumulative_sum = cumulative_sum + rainfall\n",
    "            \n",
    "            # if this row's rainfall is greater than the current max value, update with the new max\n",
    "            if rainfall > max_value:\n",
    "                max_value = rainfall\n",
    "\n",
    "            # if this row's rainfall is less than the current min value, update with the new min    \n",
    "            if rainfall < min_value:\n",
    "                min_value = rainfall\n",
    "\n",
    "    # after looping through all the rows, divide the cumulative sum by the count and round to get the mean\n",
    "    mean_value = round(cumulative_sum / count, 1)\n",
    "    \n",
    "    # print out the mean and max values\n",
    "    print('mean:', mean_value, 'inches')\n",
    "    print('max:', max_value, 'inches')\n",
    "    print('min:', min_value, 'inches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9a63a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 1\n",
    "Can you think of a straightforward way to get the median value of rainfall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35bba9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7db63511",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1.2 List-based iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4611322e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some things that are hard in the row-based iteration approach above become easier if we can store all of the values in one object, like a list.\n",
    "\n",
    "Let's try to reimplement a cleaner version of what we did above by appending the values from each row into a single list, and then performing the calculations we want on the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1220d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/rain.csv', 'r') as csvfile:\n",
    "    x = []\n",
    "    itemreader = csv.reader(csvfile)\n",
    "    \n",
    "    # Skip the header row\n",
    "    next(itemreader)\n",
    "    for row in itemreader:\n",
    "        \n",
    "        # keep only non-missing values\n",
    "        if row[1] != '':\n",
    "            x.append(float(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da80c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b3af0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "X is now a single list object with all the values in the file, whereas initially we just had each row producing one list with one element in it (from one row), and then printing that, before recreating it with the value from the next row.  The iteration approach kept only one row at a time, and we could not easily do calculations like a median.\n",
    "\n",
    "Using the list (x), we should now have an easier time with mean and median calculations, using list methods like sum and len, and sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be0497",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean_x = round(sum(x) / len(x), 1)\n",
    "print(mean_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d38693",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x.sort()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c32cbf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 2\n",
    "How can we print the value that is halfway through the list? (Tip: use `int()` and `len()` functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba2b1d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceca96fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is also simple to get the min and max, using built-in list indexes to get the first and last element from the sorted list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf9b77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "min_x = x[0]\n",
    "min_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc4a05",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "max_x = x[-1]\n",
    "max_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e2a3cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK, this is progress.  Can we now do other math on the data, like multiply each value by 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa48ffa6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = x * 5\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3608cb9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Not what we wanted. That just concatenated 5 copies of the list together! Let's try a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1c37b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "for item in range(len(x)):\n",
    "    y.append(x[item] * 5)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b311a8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alright, this is a big improvement over the iteration-by-rows approach, but it still a bit tedious, especially if we had a much larger list of items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98f78b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1.3 Vectorization: an Alternative to Iteration\n",
    "\n",
    "**Vectorization** is a more computationally efficient approach to performing repeated operations over a sequence of data. Rather than having Python perform an operation on one item at a time until it reaches the end of the sequence, we can tell Python to do it all at once. This is vast oversimplification of what's actually going on behind the scenes when we employ vectorization, but its close enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba7c848",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In Python, vectorization is made possible through the use of a data type called an `array`, which is defined by the NumPy library. We'll take a closer look at NumPy arrays in a minute, but for now let's admire how they allow us to perform the same exact analysis we just did without ever having to use a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d185ce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rain = np.array(x)\n",
    "print(round(np.mean(rain),1))\n",
    "print(np.median(rain))\n",
    "print(np.min(rain))\n",
    "print(np.max(rain))\n",
    "print(rain * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873a4677",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Much easier than coding for loops and counters, don't you think?  And a lot faster on large datasets, too. In addition to vectorization, NumPy provides us with access to more complex mathematical operations like variance and standard deviation, and integrates nicely with the most common plotting libraries you'll use in Python.\n",
    "\n",
    "Let's look at a sorted list of the elements in the array as a percentage of total rainfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7c402",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.sort(rain) / np.sum(rain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a01c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.2 The Python Scientific Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1606ef7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NumPy and Matplotlib are key components of what is commonly referred to as the **Python Scientific Stack**, which includes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f710c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- NumPy\n",
    "- SciPy\n",
    "- Matplotlib\n",
    "- Scikit-learn\n",
    "- IPython\n",
    "- pandas\n",
    "- Jupyter\n",
    "- Dask\n",
    "- NetworkX\n",
    "- Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce7bd2",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.3 NumPy Arrays\n",
    "\n",
    "The **array** is the data type which underpins all nearly all scientific computing in Python. Arrays are formally defined in the **NumPy** library, and since NumPy is not a built-in module (like `math` for example), the `array` is not a built-in data type. This means we have to install NumPy and then import it in order to use it and its many methods. Fortunately, the version of Python you installed from Anaconda came with NumPy pre-installed, so the following `import` statement should work without you having to do anything more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2bdb43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np  # we \"alias\" numpy with \"np\" to make it easier to call it later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6ebbf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If Python tells you that it can't find the NumPy module when you run the cell above, you'll have to install it yourself. To do that, use the following instructions:\n",
    "\n",
    "1. Open up a bash terminal\n",
    "2. Activate the right conda environment using `conda activate <env name>`. By default this is `base`.\n",
    "3. Run `conda install numpy`\n",
    "4. Restart your Juypter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba2157a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 3\n",
    "\n",
    "What's the difference between `Install` and `Import` for libraries in Python? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e5354a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3.1 Install vs Import\n",
    "\n",
    "Nearly all of the Python libraries for scientific computing are designed to work with arrays. This makes NumPy a **dependency** of each of them. In other words, you cannot import those other libraries unless you already have NumPy **installed** in your Python environment. This is because as soon as you do `import scipy` for example, SciPy is going to do `import numpy` behind the scenes. This means you do not need to import NumPy yourself everytime you want to use SciPy; it just needs to be _installed_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b388ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3.2 Arrays vs. Lists\n",
    "\n",
    "Arrays look a lot like `list` types, but don't be fooled: they are much more powerful. In general, an `array` allows you to _vectorize_ your calculations instead of iterating over a list and applying an operation element by element. Although they are convenient and readable, _for loops are very slow_. Especially when datasets are large, the **computational efficiency** gained by using vectorized calculations can be significant. In addition to speed, arrays are a convenient data structure for implementing the numerical methods from linear algebra and statistics which comprise the majority of applications of scientific computing applications used in data science.\n",
    "\n",
    "Let's start by creating a list, and then creating an array from that list.  Then let's compare how the list of integers works compared to the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50186059",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = list(range(1, 6))\n",
    "y = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67055079",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfaf3a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d50f6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These two objects x and y look almost the same ... but they are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f300046",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9242c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02dd7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how we can do math operations on these two versions of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42b5df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892065cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188799f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "min(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff3da4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "min(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1848ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So far so good -- not easy to tell the difference between lists and arrays...but we haven't really gotten NumPy involved yet. Let's see how we might use NumPy to perform some operations beyond the simply min/max/sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf205cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In some cases, we can use a Numpy method and apply it to a list of numbers like we have in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daeb609",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be9a26b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb0634",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.median(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93c7f2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.median(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d8129a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "np.size(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fae979",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.size(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f3ed4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c2602",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0e0c83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When you apply the `/` operator to an `array`, NumPy knows that you want to divide each element rather than the array itself. Python will not make this same assumption for a `list` type. To perform the same operation on a list, we'd have to use iteration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae549174",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xscaled = [z / 10 for z in x]\n",
    "print(xscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11bebc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For arrays/lists as small as these, the advantage of vectorization might not seem obvious. Maybe bigger objects and more complex operations might make it more clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a62030",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = list(range(1, int(1e6)))  # 1 million numbers\n",
    "y = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94484a4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%time xscaled = [(z ** 2)/ 10 for z in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade15e2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%time yscaled = (y ** 2) / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54c91c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 4\n",
    "\n",
    "How much faster is the vectorized approach than the list-based approach? Why is it so? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89eef19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3.3 Working with Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ec348",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The easiest ways to create arrays:\n",
    "1. Convert a list:\n",
    "```\n",
    "np.array([1, 2, 3])\n",
    "```\n",
    "2. Initialize an array of the shape you want filled with all zeros or all ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8ec1d",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z = np.zeros(10)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed2cf6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And we can set values in the arrays by index value -- meaning they are **mutable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504c8b5",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z[4] = 1\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc39911",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Arrays are not always simple one-dimensional arrays like a column in a spreadsheet.  They could be 2 dimensional like a table, or 3 dimensional like a set of tables, or many dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c42cc1",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z = np.arange(9).reshape(3, 3)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d247191",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 5\n",
    "What do we get with the following line of code? How can we sum up row or column values? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58afc730",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(Z[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d719d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`np.arange()`? `np.reshape()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e366c61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of course if you have a 2-dimensional array, your indexing into the array becomes two dimensional as well, with row, then column index values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd2095",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z[0, 2] = 9\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe8f43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7166e0",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3b258",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You might or might not have noticed that when we do calculations on arrays like adding two arrays together, the default behavior is element by element.  Look at the result of adding Z to itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad9063",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680c01a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or multiplying it by itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8ceaf",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z * Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac850a42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this example we create a 10 x 10 array of random numbers and find the min and max of the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e22647",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z = np.random.randint(0, 100, (10, 10))\n",
    "Zmin, Zmax = Z.min(), Z.max()\n",
    "print(Zmin, Zmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda0b0d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some NumPy operations are available as array instance methods. But others are only defined as NumPy functions, like median and percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7b22b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(Z))  # function\n",
    "print(Z.mean())  # method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938dfb2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(np.median(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2861a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(Z.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad60a22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember this example from the first class?  It was using Numpy arrays and the Matplotlib library for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857a482",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeea5a2",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = range(100)\n",
    "y = np.sin(x)\n",
    "plt.plot(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7131786",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3.4 Numpy for Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007ce9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you have used a program like Matlab, R, Gauss, Octave or any other matrix - based language for doing linear algebra or statistics, this is not what you expect.  Instead, multiplying two matrices would be expected to produce a dot product, or matrix multiplication.  NumPy can do that too, but it just uses a different syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c3259",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z = np.arange(9).reshape(3, 3)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec4d9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z.dot(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca8c4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can also do a matrix transpose (switching axes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc5bbf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.array_equal(np.transpose(Z), Z.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f2fca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And easily compute an identity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f856fc0",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.eye(3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabef5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 6\n",
    "What is the difference between `Z * Z` and `Z.dot(Z)`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e3a6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3.5 NumPy Summary\n",
    "\n",
    "NumPy is a very powerful multi-dimensional array processing library for Python, and it is very fast because the underlying implementation is actually in the C programming language.\n",
    "\n",
    "The Scientific Python ecosystem we will be using in this course uses NumPy heavily, but usually it is \"under the hood\", and we use it through the Pandas library which makes it much easier to use and to handle data as tables.  But you might find significant value in learning more about NumPy if you need lower level functionality or want to code something very computationally intensive. It is not expected that you use it heavily in this course, however.\n",
    "\n",
    "As we will learn later in the course, we can use the NumPy multi-dimensional array library and other libraries built on it to do statistical analysis and machine learning. NumPy solves one of the main limitations of the Python language, which is that it is very slow at iterating or looping through calculations once the size of the data becomes large. But NumPy does those same operations very efficiently by using vectorized computation. For example, it multiplies two arrays all at once rather than looping through each row and multiplying the elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64596c02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3.6 Exercise\n",
    "\n",
    "Now do some experimenting with Numpy to get more familiar with it:\n",
    "\n",
    "- Try creating a 100 x 100 element array with random numbers\n",
    "\n",
    "- Multiply those by 2\n",
    "\n",
    "- Calculate the 75% percentile value of the array\n",
    "\n",
    "- Calculate minimum, maximum, median, and 10th and 90th percentile values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4fb375",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52c18206",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.4 Graphing with `matplotlib`\n",
    "The `matplotlib` library includes a variety of functions that allow us to build plots of data. There are lots of different data viz libraries in Python we'll be exploring later, but most of them are built on top of Matplotlib (kind of like NumPy for data analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed310e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " Once again, you must first import the library before you can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f0ee5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb7441",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before you can use the plotting functions, you must first have some data to plot. Below are some data on Berkeley restaurants taken from Yelp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cadbc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "restaurants = [\"Gypsy's\", \"Tacos Sinaloa\", \"Sliver\", \"Muracci's\", \"Brazil Cafe\", \"Thai Basil\"]   # <-- Are these still open?\n",
    "rating = [4, 4, 4, 3.5, 4.5, 3.5]\n",
    "number_of_ratings = [1666, 347, 1308, 294, 1246, 904]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c7fc4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You may be interested in seeing if there is a relationship between the number of ratings a restaurant has and their rating out 5 stars. It is difficult to determine this from looking at the numbers directly, so a plot can come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eaf6ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(number_of_ratings, rating)  # create a scatter plot\n",
    "plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc2982",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Out of context, this plot is not very helpful because it doesn't have axis labels or a title. These components can be added using other `matplotlib` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20093f7a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(number_of_ratings, rating)  # create a scatter plot\n",
    "plt.xlabel(\"Number of Ratings\")  # add the x-axis label\n",
    "plt.ylabel(\"Star Rating (out of 5)\")  # add the y-axis label\n",
    "plt.title(\"Berkeley Restaurant Star Ratings by Number of Ratings\")  # add a title\n",
    "plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed843cc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are many other attributes you can add to plots and many more types of plots you can create using this library. For a comprehensive description, visit the [documentation](https://matplotlib.org/api/pyplot_api.html)! Included here are some basic plots that you may find useful for this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0dd869",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create a bar plot\n",
    "plt.bar(restaurants, number_of_ratings)\n",
    "\n",
    "# add the x-axis label\n",
    "plt.xlabel(\"Restaurant\")\n",
    "\n",
    "# add the y-axis label\n",
    "plt.ylabel(\"Number of Ratings\")\n",
    "\n",
    "# add a title\n",
    "plt.title(\"Berkeley Restaurant Star Ratings\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14181f20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create a histogram\n",
    "plt.hist(rating)\n",
    "\n",
    "# add the x-axis label\n",
    "plt.xlabel(\"Star Rating (out of 5)\")\n",
    "\n",
    "# add the y-axis label\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# add a title\n",
    "plt.title(\"Berkeley Restaurant Star Rating Frequencies\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d65d84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is also possible to overlay multiple lines on a single plot. Below are some made up data about the number of people with each height in two different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e7d41",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "height = [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]\n",
    "class_one = [1, 1, 0, 3, 7, 4, 3, 7, 8, 3, 1, 2, 1]\n",
    "class_two = [0, 0, 3, 1, 3, 4, 1, 2, 6, 2, 8, 5, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c741df4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can use the `.plot()` function to plot both of these functions as line plots. If you run multiple calls to plotting functions in the same cell, Python will simply layer the resulting plots on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad07dd5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create a line plot for the first class\n",
    "# label this line as class one for the legend\n",
    "plt.plot(height, class_one, label = \"class one\")\n",
    "\n",
    "# create a line plot for the second class\n",
    "# label this line as class two for the legend\n",
    "plt.plot(height, class_two, label = \"class two\")\n",
    "\n",
    "# add the x-axis label\n",
    "plt.xlabel(\"Height (in)\")\n",
    "\n",
    "# add the y-axis label\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# add the legend\n",
    "plt.legend()\n",
    "\n",
    "# add the title\n",
    "plt.title(\"Frequencies of Different Heights for Two Classes\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6d92a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.5 Intro to pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbad2e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`pandas` is the most popular library for doing data analysis in Python. The Wes McKinney book you've been reading is basically just a guide to Pandas. I wince every time I have to say the word \"pandas\" because I think it's a dumb name, but I wind up having to wince a lot because it is so fundamental to what I use Python for on a day-to-day basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fdb893",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Just like NumPy, we'll import pandas and give it the alias `pd`. This is just convention. You could give it any alias you like, or no alias at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83336ba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012a516",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.5.1 The `pandas` data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fe8d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "The pandas library provides us with two new data types, both of which are built on the NumPy `array`:\n",
    "1. `pandas.Series`: a 1d named array with a named index.\n",
    "2. `pandas.DataFrame`: a 2d array (table) of `pandas.Series` columns which share a named index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d2de5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Series` objects are basically dictionaries, where the keys are now an index and the values are an array of all the same type. A `DataFrame` is like a table where the values are a 2d array or matrix and each column is comprised of a `Series`.\n",
    "\n",
    "You can create a `Series` from a list or a DataFrame from a list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad2409",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(height, index=['person_' + str(i) for i in range(len(height))], name='height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68e641",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [height, class_one, class_two],\n",
    "    index=['height','class_one', 'class_two'],\n",
    "    columns=['person_' + str(i) for i in range(len(height))]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1a45d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe9fb5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.5.2 Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581f3e18",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Typically, however, we'll create DataFrames by reading in data that is already in a tabular format, like a .csv. Let's look at our rain data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd76bb9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/rain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f391aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2714f527",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice a few things that have happened here.\n",
    "\n",
    "1. `pd.read_csv()` had enough built-in smarts to read the first row of the file as a header.\n",
    "1. It then read all rows in the file at once to create a table of NumPy arrays.\n",
    "1. It created an automatic unique index, beginning with zero.\n",
    "1. It inferred the type of each variable from the data.\n",
    "\n",
    "Let's explore this pandas `DataFrame` to learn some of its features. Note that a pandas Series is like one column of this DataFrame, coupled with its own index column. So the main difference between a Series and a DataFrame is that the latter has multiple columns. Columns can be of different data types, but within a column, must be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd95bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cde875",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a368a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522aa287",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "We can select subsets of the rows by indexing, and select specific columns by their name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c810114",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['rainfall_inches'][:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0586588",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can get all of our statistics on the rainfall_inches column in one short command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf209c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['rainfall_inches'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27afd389",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice how it silently handled the missing value for September and gave the correct statistical results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb66b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can also get these values 'a la carte'.  You might recognize that these are essentially NumPy functions which Pandas has provided access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61daaadf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['rainfall_inches'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f046593f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['rainfall_inches'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80a49e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['rainfall_inches'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c2662",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that there is some flexibility in syntax, with multiple ways to perform certain operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f8868",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['rainfall_inches'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9bd07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.rainfall_inches.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d57db4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.min(df.rainfall_inches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a234b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pandas also provides access to some of the basic functionality of `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dfadb8",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.plot(y = 'rainfall_inches', x = 'month_2014')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9184a63c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.5.3 Slicing and Selecting Data\n",
    "\n",
    "#### 2.5.3.1 Indexing\n",
    "\n",
    "pandas provides two indexing methods for getting slices of rows and columns from your data: `loc` and `iloc`. Both of them use the same square bracket notation you should be used to by now:\n",
    "\n",
    "```\n",
    "[<row index>, <column index>]\n",
    "```\n",
    "\n",
    "The `iloc` method lets you index a pandas `DataFrame` like indexing a 2d NumPy `array`, using integers to represent the _positions_ of rows and columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354d81b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[0:6, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe0bca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `loc` method lets us use the index _labels_ to get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79c87f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[0:6, :'rainfall_inches']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7fa9fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since `loc` does not use positional indexing, notice how the rules of positional indexing do not apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3b182f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That's right, label-based slicing is **endpoint inclusive**. More on this [here](https://pandas.pydata.org/docs/user_guide/advanced.html#advanced-endpoints-are-inclusive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c306521",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sometimes it's useful to give your index a meaningful name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c526d8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.index.name = 'obs_id'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eea6e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.5.3.2 Boolean Indexing\n",
    "\n",
    "You can easily filter rows of a dataframe by applying one or more boolean expressions to values in one or more columns. Below we filter `df` to select only months with less than 4 inches of rainfall.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1adeff",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df[df['rainfall_inches'] < 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e98d45",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice the nested use of df.  What happens if you don't do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39b9b3",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['rainfall_inches'] < 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772e39f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can also select rows based on the values of more than one column.  Just remember to nest the individual conditions within parentheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f093522",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df[(df['month_2014'] == 'jan') & (df['rainfall_inches'] > 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58984159",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "pandas lets us get pretty fancy with our boolean expressions to extract exactly the data we want from our table.\n",
    "\n",
    "For example, we can filter based on string matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348e384",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df[df['month_2014'].str.contains('j')]  # more on this later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ad251",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can filter out missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c12d8",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df[df['rainfall_inches'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea5e04",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or do the opposite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989da172",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df[df['obs_id'].isin([5,6,7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec768ae1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df[df['rainfall_inches'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7758e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we know that the value for \"sep\" should be 2.5. We can use `loc` to perform assignment in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53850c9d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['month_2014'] == 'sep', 'rainfall_inches'] = 2.5  # boolean row-index + label-based col index\n",
    "df[df['rainfall_inches'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de73a72",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we want to preserve this change, we can save the DataFrame to to disk as a new .csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6cdf2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('data/rain_updated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b78e79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.5.4 String functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd1f08",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We briefly saw an example of string matching above. pandas comes with a number of very powerful functions more operating on `str` type Series. Let's investigate them using some California Census data I've already prepared for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a443fec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "census_df = pd.read_csv('data/ca_tracts_pop_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc989503",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "census_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854073b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's create a new column, called \"state\", and populate it by getting the last element in the \"geodisplay\" field. This is a very common type of data processing you might need to do on data you acquire for your own projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feef792",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "census_df['state'] = census_df['geodisplay'].str.split(',').str[2]\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4d10b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We use the same approach to add a 'county' column to our dataframe, pulling the values from the geodisplay column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7869c6",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "census_df['county'] = census_df['geodisplay'].str.split(',').str[1].str.replace(' County', '')\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4c111",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now let's see what the full list of unique county names is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8ce76",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "census_df['county'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78bcd1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's create a census tract column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae643a",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "census_df['Tract'] = census_df['geodisplay'].str.split(',').str[0].str.split().str[2]\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d210d7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Could you have extracted the Tract number from the \"GEOIDLONG\" column instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d40316",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "census_df['Tract'] = census_df['GEOIDLONG'].str[-6:-2] + '.' + census_df['GEOIDLONG'].str[-2:]\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598aa818",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can you spot the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8bfa4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.5.5 More pandas operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505b5f2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's go back to our rainfall data\n",
    "\n",
    "**Summary stats**\n",
    "pandas makes it really easy to quickly summarize and parse your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13530aa2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.head()  # first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3232a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(3)  # random 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461703f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1377f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Sorting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb5e2c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values(['rainfall_inches'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8012dce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Basic Aggregations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5957f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['month_2014'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9ef6b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['month_2014'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a1dd7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['month_2014'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e92ee05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember, DataFrames are just NumPy arrays with some frills on top. The `array` form is stored as an **attribute** of the DataFrame named \"values\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8fe97",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(df.values)\n",
    "print(type(df.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b8074",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Column selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb90ba8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can also easily create a DataFrame from a dictionary. Let's follow an example from the Python for Data Analysis book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e6c2b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n",
    "    'year': [2000, 2001, 2002, 2001, 2002],\n",
    "    'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\n",
    "sp = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df37e41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's investigate the contents of `sp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa3747",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d55f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e32d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can refer to a column in two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50e963",
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sp['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243bb01e",
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sp.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb066a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 7\n",
    "Generally it is preferable to be explicit and use the `df[<column name>]` approach. Can you imagine why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596ea62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if we had a column named \"values\"? How could pandas distinguish between \"values\" the attribute and \"values\" the column?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c78bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e395f53",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can also iterate through the rows of a dataframes using `iterrows()`. This works a lot like list enumeration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c190a31",
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i, row in sp.iterrows():\n",
    "    if row['state'] == 'Ohio':\n",
    "        print(row['state'])\n",
    "    else:\n",
    "        print('Row {0} is not Ohio'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d0a4e0",
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://i.kym-cdn.com/photos/images/original/001/733/249/7a0\" alt=\"drawing\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2f028",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 2.6 Exercises\n",
    "\n",
    "### 2.6.1 Pandas expressions\n",
    "Below are a series of questions, with the answers remaining for you to fill in by using pandas expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f082b3",
   "metadata": {},
   "source": [
    "Can you get a profile of a column that is not numeric, like state? Try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa08bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67df6292",
   "metadata": {},
   "source": [
    "How can we print the data types of each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b2d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4663182",
   "metadata": {},
   "source": [
    "How can we print just the column containing state names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee5f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "800baa5d",
   "metadata": {},
   "source": [
    "How can we get a list of the states in the DataFrame, without duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e2d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "876be48b",
   "metadata": {},
   "source": [
    "How can we compute the mean of population across all the rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6fa796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c29c1f73",
   "metadata": {},
   "source": [
    "How can we compute the maximum population across all the rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e68a787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e562d2b",
   "metadata": {},
   "source": [
    "How can we compute the 20th percentile value of population? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362fb68a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10f51b24",
   "metadata": {},
   "source": [
    "How can we compute a Boolean array indicating whether the state is 'Ohio'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ffc19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a966dc5f",
   "metadata": {},
   "source": [
    "How can we select and print just the rows for Ohio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c220a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1191112",
   "metadata": {},
   "source": [
    "How can we create a new DataFrame containing only the Ohio records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca46404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ffdd32c",
   "metadata": {},
   "source": [
    "How can we select and print just the rows in which population is more than 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd45250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04ac9ea8",
   "metadata": {},
   "source": [
    "How could we compute the mean of population that is in Ohio, averaging across years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a94689a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf708513",
   "metadata": {},
   "source": [
    "How can we print the DataFrame, sorted by State and within State, by Population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da301d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "824c5452",
   "metadata": {},
   "source": [
    "How can we print the row for Ohio, 2002, selecting on its values (not on row and column indexes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64da333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "595092b0",
   "metadata": {},
   "source": [
    "How can we use row and column indexing to set the population of Ohio in 2002 to 3.4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81974332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b12d547",
   "metadata": {},
   "source": [
    "### 2.6.2 COVID cases from NY Times\n",
    "\n",
    "We end our intro to pandas with a quick look at how easy it is to load data into a dataframe. In this case, let's pull data from the New York Times online data for COVID cases, by county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a3b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv'\n",
    "uscovid = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab1ef9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uscovid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacovid = uscovid[uscovid['state']=='California']\n",
    "cacovid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0bc85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alameda_covid = cacovid[cacovid['county']=='Alameda']\n",
    "alameda_covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34920290",
   "metadata": {},
   "source": [
    "Go ahead, experiment with these data - see what you can do with some basic Pandas commands.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc73fcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4eb4858e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Intro to Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af781fee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Merges and Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b1202",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pandas has a very powerful set of methods for merging DataFrames together. This is a very common procedure as it is very rare for all of your data to be stored in a single file. These methods are designed to work like database queries, so the syntax borrows heavily from SQL. If you're unfamiliar with relational algebra/SQL, check out the official pandas [docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#brief-primer-on-merge-methods-relational-algebra) for more detail. Let's review them using a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7dc06a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "favorite_numbers = pd.DataFrame([\n",
    "    [\"Boba Fett\", 42],\n",
    "    [\"Boba Fett\", 3.14],\n",
    "    [\"Bossk\", 7],\n",
    "    [\"Bossk\", 9],\n",
    "    [\"IG-88\", 3],\n",
    "    [\"Dengar\", np.NaN]], columns=[\"Name\", \"Number\"])\n",
    "\n",
    "email_addr = pd.DataFrame([\n",
    "    [\"Boba Fett\", \"boba@bountyhunters.com\"],\n",
    "    [\"Bossk\", \"boskk@bountyhunters.com\"],\n",
    "    [\"IG-88\", \"ig88@bountyhunters.com\"],\n",
    "    [\"Dengar\", \"dengar@bountyhunters.com\"],\n",
    "    [\"4-LOM\", \"4lom@bountyhunters.com\"],\n",
    "    [\"Zuckuss\", \"thezuck@bountyhunters.com\"]], columns=[\"Name\", \"Email\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa2535",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first table contains favorite numbers of some famous people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf4064",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://www.syfy.com/sites/syfy/files/2020/05/bounty-hunters-esb.jpg\" width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b14112",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "favorite_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666d2df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The second table contains the individuals email addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a9622",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "email_addr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8762263",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are actually many ways you could imagine combining data from both of these tables.  In the following we work through a few example methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61fef5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1.1 Merge\n",
    "\n",
    "Probably the most general and standard way to join tables in pandas is to use the merge function. There are many ways to do a merge, however, so let's go through them each one by one:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc823d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.1.1.1 Inner merge\n",
    "The default type of `merge` is the **inner** merge, which corresponds to an \"inner join\" in SQL. Each merge type requires you to specify the column(s) on which you are joining. An inner join, however, only returns rows where the `on` value appears in both tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34dd90d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(email_addr, favorite_numbers, on=\"Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be74b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that Boba Fett and Bossk occur 2 times since each had two favorite numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c57df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.1.1.2 Left merge\n",
    "\n",
    "A **left merge** will keep all the entries in the left table even if they have no matching entry in the right table.  For example, we don't have 4-LOM's or Zuckess's favorite numbers, so the inner join simply dropped these rows. With a _left_ join, we preserve all rows from the lefthand table, with unmatched rows appearing as missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039d99f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(email_addr, favorite_numbers, on=\"Name\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b614bb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = pd.merge(email_addr, favorite_numbers, on=\"Name\", how=\"left\")\n",
    "b = pd.merge(favorite_numbers, email_addr, on=\"Name\", how=\"right\")\n",
    "all(a.index == b.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d84150",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.1.1.3 Outer Merge\n",
    "\n",
    "The outer join keeps entries in both tables even if they don't have a match in the other and substitutes NaN for missing values. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ef155",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(email_addr, favorite_numbers, on=\"Name\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f27425",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1.2 Join\n",
    "\n",
    "Pandas also provides a join function which joins two tables on their index.  This function also let's you specify what kind of join you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706bdf1f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "favorite_numbers.set_index(\"Name\", inplace=True)\n",
    "email_addr = email_addr.set_index(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67aaae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "email_addr.join(favorite_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f85849",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note the default join type is `left`, rather than inner. I rarely use `join()` FWIW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa01a7",
   "metadata": {},
   "source": [
    "### 3.1.3 More resources to learn about merges and joins\n",
    "\n",
    "These are powerful tools and there are some subtleties in using them.  I recommend doing a fair amount of reading and a lot of practicing to get these ideas and the accompanying syntax internalized into your programming toolkit.\n",
    "\n",
    "Here is another good (and short) tutorial: https://chrisalbon.com/python/pandas_join_merge_dataframe.html\n",
    "\n",
    "And as always, read the docs: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e08793",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2 Using Merge and `str` ops on Real Data\n",
    "\n",
    "Now that we have some concepts and syntax under control, let's try using them on some larger data tables from the Census, and learn how to use string operations to work with these data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758d11e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 3.2.1 Importing County level Census Data for the U.S.\n",
    "\n",
    "Lets say we want to analyze county level population trends. I downloaded two population estimates tables for U.S. Counties and provide them for you here. One contains 2000-2010 data, and the other has 2010-2016.  Lets load them and do some data exploration and manipulation to get a merged county level population series for the 2000 - 2016 period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb97013",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "co00 = pd.read_csv('data/co-est00int-tot.csv', encoding='latin1')\n",
    "co00.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed430c",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "co16 = pd.read_csv('data/co-est2016-alldata.csv', encoding='latin1')\n",
    "co16.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a15135",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.2.1.1 A note on character encodings\n",
    "One thing that you will encounter as you read data from various sources is that the **character encoding** might be unusual, and require setting the encoding on the read statement.  Full documentation on this is available here:\n",
    "https://docs.python.org/3/library/codecs.html#standard-encodings\n",
    "\n",
    "Some of the most common ones are:\n",
    "\n",
    "- latin1\n",
    "- iso-8859-1\n",
    "- utf_8\n",
    "- utf_16\n",
    "- utf_32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72a4e9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see what happens when you try loading a csv file with an encoding issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79668c02",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "co00_wrong = pd.read_csv('data/co-est00int-tot.csv')\n",
    "co00_wrong.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d82ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pandas will try to use `utf-8` to parse your data by default, and if it has trouble it will throw an error and tell you exactly which character caused it. Trying the `latin1` codec is always a good next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e62f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2.2 Data Exploration and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23855678",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's look at the columns in our dataframe now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4eca8",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "list(co16.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d7e09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "co16.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942e9f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "116 columns!?!  We don't want all of those!  How can we keep just the columns we want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ecb9d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "co16s = co16.loc[:, :'POPESTIMATE2016']\n",
    "co16s.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c7507",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ahhh, that's better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3c801",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "co16s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99020a54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But wait... what's going on here?  There seem to be at least two SUMLEV values and it looks suspiciously like there are State level summaries of `POPESTIMATES` embedded in this County-level file.  Let's check how many records there are for each SUMLEV in the file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ed747",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "co16s['SUMLEV'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cd205",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hmm, OK, so we have 51 SUMLEV 40, which seem to be the State estimates, plus one... let's pull those into a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ad75f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "st00 = co00[co00['SUMLEV']==40]\n",
    "st00.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c17e70",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "st00.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f2e90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So what is that 51st entry in the set of States?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871681e5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "st00['STNAME'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4123461e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's pull the state records out of the 2016 file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d48952",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "st16 = co16s[co16s['SUMLEV']==40]\n",
    "st16.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaad386",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 3.2.3 Merging\n",
    "\n",
    "...and merge the 2000 and 2016 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7406f40",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stjoin = pd.merge(st00, st16, on='STATE')\n",
    "stjoin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf755a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.2.3.1 Dealing with duplicate columns\n",
    "Looks like it worked! But what are all those columns with `_x` and `_y` suffixes?\n",
    "\n",
    "Seems like a mess to keep all those duplicate columns and have them get renamed like this... what to do...?\n",
    "\n",
    "Maybe we could find the columns that are different in the second dataframe and just add those to the first dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e32fa0",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_use = list(st16.columns.difference(st00.columns))\n",
    "cols_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c58847",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Anyone have an alternative solution? Maybe one that involves list comprehension?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5188df4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Would it work if we tried to join these two using this as the list of columns from ST16? What about the join column? Seems like that would be pretty handy to have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f637c56",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_use.append('STATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8cf0f1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we can do a much cleaner merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9155e14",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stjoin2 = pd.merge(st00, st16[cols_to_use], on='STATE')\n",
    "stjoin2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bd6f98",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looking pretty good.  But there are still a lot of columns we really don't want.  Let's drop those, and set the index to STNAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44806c66",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stjoin3 = stjoin2.drop(\n",
    "    ['STATE', 'SUMLEV','REGION','DIVISION','COUNTY','ESTIMATESBASE2000','ESTIMATESBASE2010','CENSUS2010POP','CTYNAME'],\n",
    "    axis=1).copy()\n",
    "stjoin3.set_index('STNAME', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5162ed5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Much nicer.  But I really don't like those column names.  Can't we just make them simple years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ae8f6",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stjoin4 = stjoin3.copy()\n",
    "stjoin4.columns = list(range(2000, 2017))\n",
    "stjoin4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d11ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Maybe we should write our updated cleaned up data back to a CSV file to reuse later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d6095",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stjoin4.to_csv('data/statepop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64505871",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It might also be handy to transpose the data so that the rows become columns and vice versa.  Let's do that and select just a couple of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b3c06",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ST = stjoin4.transpose()\n",
    "ST.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd898a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Maybe a quick plot of the population trends in these states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0a034",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ST['California'].plot(figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc2fd2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.3 More Data Exploration and Cleaning: Berkeley PD Calls\n",
    "\n",
    "Let's load some police call data from the Berkeley Police Department and explore those data... this is an example of having to wrangle some messy, real world data. It's advanced work that you now can handle in Python!\n",
    "\n",
    "For example lets say we want to find out how many vandalism calls there are each day of the week. Let's load some data and figure out how to answer that question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce889cd",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "calls = pd.read_json(\"https://data.cityofberkeley.info/resource/k2nh-s5h5.json\")  # json\n",
    "calls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42654f52",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "calls.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a1fdf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.3.1 Preliminary observations on the data?\n",
    "\n",
    "1. `eventdt` -- Contain the incorrect time stamp (all the times are 12:00 am)\n",
    "1. `eventtm` -- Contains the time in 24 hour format (What timezone?)\n",
    "1. `indbdate` -- Appears to be correctly formatted and appears awfully consistent in time.\n",
    "1. `block_location` -- Errr, what a mess!  newline characters, and Geocoordinates all merged!!  Fortunately, this field was \"quoted\" otherwise we would have had trouble parsing the file. (why?)\n",
    "1. `blkaddr` -- This appears to be the address in Block Location.\n",
    "1. `city` and `state` seem redundant given this is supposed to be the city of Berkeley dataset.\n",
    "1. `cvdow` almost gives us what we want (\"dow\" == Day of Week), but it would be nice to have the actual day names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ddb878",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.3.2 Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a78a1a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How could we find out how many Vandalism calls happen by day of the week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9a318",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "calls['eventdt'] = pd.to_datetime(calls.eventdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c2e466",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "calls['day_of_week'] = calls.eventdt.dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9d0b3",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "calls.loc[calls['offense'] == 'VANDALISM', 'day_of_week'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc999f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.3.3 Datetime operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ba3aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To complete the preceding analysis, we made use of some built-in functionality in pandas for working with dates. The first thing we did was to convert the `eventdt` column from a `str` type to a `datetime` type. Once we have a `datetime` type column, pandas makes it easy to perform all kinds of operations. Note: this syntax should look familiar to you from when we saw `str` type operations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7a46c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "calls['eventdt'].dt.year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3867339e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "calls['eventdt'].dt.day_of_year.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e51052",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "calls['eventdt'].dt.days_in_month.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390d312",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. For next time \n",
    "- Work through the exercises in section 2.6 of this notebook\n",
    "- Get started on Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a0679",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401d76b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Sources\n",
    "\n",
    "This notebook was heavily adapted from previous course material by Prof. Paul Waddell and Samuel Maurer."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "757px",
    "left": "213px",
    "top": "111.133px",
    "width": "206.333px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
